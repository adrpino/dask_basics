{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kernel-adrian/anaconda3/lib/python3.5/site-packages/dask/imperative.py:5: UserWarning: dask.imperative has been moved to dask.delayed\n",
      "  warn(\"dask.imperative has been moved to dask.delayed\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.imperative import delayed\n",
    "from dask.diagnostics import ProgressBar\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will show a progress bar on all operations of dask \n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This creates a disk cache for operations that spill out of disk\n",
    "import chest\n",
    "chest= chest.Chest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data, easy as pie\n",
    "If you know **pandas** you'll be right at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "train = dd.read_csv('data/train.csv', parse_dates=['date_time'])\n",
    "test = dd.read_csv('data/test.csv', parse_dates=['date_time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That happened very quick, didn't it?\n",
    "That's because dask execution is *lazy*, nothing actually happened when you create a *dask.dataframe* instance. It does not go and look up for the data until you do some calculation. What it did was create the structure of the dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.DataFrame<from-de..., npartitions=122>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_name</th>\n",
       "      <th>posa_continent</th>\n",
       "      <th>user_location_country</th>\n",
       "      <th>user_location_region</th>\n",
       "      <th>user_location_city</th>\n",
       "      <th>orig_destination_distance</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_mobile</th>\n",
       "      <th>is_package</th>\n",
       "      <th>...</th>\n",
       "      <th>srch_children_cnt</th>\n",
       "      <th>srch_rm_cnt</th>\n",
       "      <th>srch_destination_id</th>\n",
       "      <th>srch_destination_type_id</th>\n",
       "      <th>is_booking</th>\n",
       "      <th>cnt</th>\n",
       "      <th>hotel_continent</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>hotel_market</th>\n",
       "      <th>hotel_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-08-11 07:46:59</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>348</td>\n",
       "      <td>48862</td>\n",
       "      <td>2234.2641</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8250</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-08-11 08:22:12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>348</td>\n",
       "      <td>48862</td>\n",
       "      <td>2234.2641</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8250</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-08-11 08:24:33</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>348</td>\n",
       "      <td>48862</td>\n",
       "      <td>2234.2641</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8250</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-08-09 18:05:16</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>442</td>\n",
       "      <td>35390</td>\n",
       "      <td>913.1932</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-08-09 18:08:18</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>442</td>\n",
       "      <td>35390</td>\n",
       "      <td>913.6259</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_time  site_name  posa_continent  user_location_country  \\\n",
       "0 2014-08-11 07:46:59          2               3                     66   \n",
       "1 2014-08-11 08:22:12          2               3                     66   \n",
       "2 2014-08-11 08:24:33          2               3                     66   \n",
       "3 2014-08-09 18:05:16          2               3                     66   \n",
       "4 2014-08-09 18:08:18          2               3                     66   \n",
       "\n",
       "   user_location_region  user_location_city  orig_destination_distance  \\\n",
       "0                   348               48862                  2234.2641   \n",
       "1                   348               48862                  2234.2641   \n",
       "2                   348               48862                  2234.2641   \n",
       "3                   442               35390                   913.1932   \n",
       "4                   442               35390                   913.6259   \n",
       "\n",
       "   user_id  is_mobile  is_package      ...        srch_children_cnt  \\\n",
       "0       12          0           1      ...                        0   \n",
       "1       12          0           1      ...                        0   \n",
       "2       12          0           0      ...                        0   \n",
       "3       93          0           0      ...                        0   \n",
       "4       93          0           0      ...                        0   \n",
       "\n",
       "  srch_rm_cnt srch_destination_id  srch_destination_type_id  is_booking  cnt  \\\n",
       "0           1                8250                         1         0.0  3.0   \n",
       "1           1                8250                         1         1.0  1.0   \n",
       "2           1                8250                         1         0.0  1.0   \n",
       "3           1               14984                         1         0.0  1.0   \n",
       "4           1               14984                         1         0.0  1.0   \n",
       "\n",
       "   hotel_continent  hotel_country  hotel_market  hotel_cluster  \n",
       "0              2.0           50.0         628.0            1.0  \n",
       "1              2.0           50.0         628.0            1.0  \n",
       "2              2.0           50.0         628.0            1.0  \n",
       "3              2.0           50.0        1457.0           80.0  \n",
       "4              2.0           50.0        1457.0           21.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorize some vars:\n",
    "train.srch_adults_cnt = train.srch_adults_cnt.astype('category')\n",
    "train.srch_children_cnt = train.srch_children_cnt.astype('category')\n",
    "train.is_mobile = train.is_mobile.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby(['srch_adults_cnt','srch_children_cnt']).user_id.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How many distinct countries do we have\n",
    "train.user_location_country.value_counts().compute().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# At what time of the day people make hotel requests?\n",
    "result = train.groupby(train.date_time.dt.time).user_id.count()\n",
    "res=result.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = train.groupby(train.date_time.dt.date).user_id.count()\n",
    "res=result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indices\n",
    "We saw the performance of dask isn't _that_ good. Can we hack around to improve it?\n",
    "Let's try with indices. This is the variable that defines **partititions** in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ind=train\n",
    "train_ind.set_index(train_ind.date_time.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result1 = train_ind.groupby(train_ind.date_time.dt.time).user_id.count()\n",
    "result1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result2 = train_ind.groupby(train_ind.date_time.dt.date).user_id.count()\n",
    "res=result2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del result, result2, train_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "**dask** provides an experimental freature called *opportunistic caching*, that intelligently stores intermediate results for faster access later.\n",
    "Bear in mind that this feature is *experimental*, dask may underestimate the size of an object in memory, and when loaded, you may enjoy some swapping fun.\n",
    "\n",
    "The approach used is distinct from the *Least Recently Used (LRU)* approach used in **Spark**.\n",
    "The philosophy is more about keeping cached what you'll be needing in analytic computations, i.e.:\n",
    "- Expensive to recompute (in seconds)\n",
    "- Cheap to store (in bytes)\n",
    "- Frequently used\n",
    "- Recenty used\n",
    "\n",
    "If you want to know more: \n",
    "- https://en.wikipedia.org/wiki/Cache_algorithms\n",
    "- https://dask.pydata.org/en/latest/caching.html\n",
    "- http://matthewrocklin.com/blog/work/2015/08/03/Caching\n",
    "- https://github.com/blaze/cachey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's perform some ops on the same variable\n",
    "max_dis=train.orig_destination_distance.max().compute()\n",
    "min_dis=train.orig_destination_distance.min().compute()\n",
    "mean_dis=train.orig_destination_distance.mean().compute()\n",
    "print(min_dis, max_dis, mean_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's cache \n",
    "# you need package cachey installed to do this\n",
    "from dask.cache import Cache\n",
    "cache = Cache(2e9)\n",
    "cache.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat the operation\n",
    "max_dis=train.orig_destination_distance.max().compute()\n",
    "min_dis=train.orig_destination_distance.min().compute()\n",
    "mean_dis=train.orig_destination_distance.mean().compute()\n",
    "print(max_dis, min_dis, mean_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing data\n",
    "dask provides convenient methods to store data on common formats, such as *csv* and *hdf*.\n",
    "Besides, it supports its own format called *castra*. It's two main characteristics:\n",
    "- It's **chunked**: Data is splitted in order to apply methods on parallel.\n",
    "- It's **columnar**: when you load a csv, to retrieve a column of this file you have to process the whole file and split lines. With a *columnar* format, you can just load the column you're interested. Most analytic operations just require some of the columns of a datasets, not all of them, so processing them is a waste of time.\n",
    "\n",
    "Running this example you see that writing to *hdf* is ridiculously faster, but this happens because the first time of the write, it had to scan the files, and the second it didn't.\n",
    "This happens even though caching is *not* activated in dask. This may be because caching ocurring at OS level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 23min 37.0s\n"
     ]
    }
   ],
   "source": [
    "train.to_csv('data/dask_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  6min 23.3s\n"
     ]
    }
   ],
   "source": [
    "train.to_hdf('data/dask_output.hdf5', key='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.DataFrame<from-de..., npartitions=122>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=dd.concat(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Arrays\n",
    "Probably the most successful part of the project but maybe with least application on Kernel/LR in itself.\n",
    "It's the on-disk counterpart of **Numpy** (not the whole API, just the most common functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_array(array_name='random.hdf5', size=1000000000):\n",
    "    import numpy as np\n",
    "    np.random.seed(0)\n",
    "    if os.path.exists(array_name):\n",
    "        return None\n",
    "        \n",
    "    print(\"Store random array in %s\" % array_name)\n",
    "    import h5py\n",
    "\n",
    "    with h5py.File(os.path.join('data',array_name)) as f:\n",
    "        dset = f.create_dataset('/x', shape=(size,), dtype='f4')\n",
    "        for i in range(0, size, 1000000):\n",
    "            dset[i: i + 1000000] = np.random.exponential(size=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store random array in random2.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Let's create 2 arrays of 1 billion (1e9) elements.\n",
    "# Don't try this in memory\n",
    "random_array('random1.hdf5', size=int(2e9))\n",
    "random_array('random2.hdf5', size=int(2e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's load this into dask arrays.\n",
    "import h5py\n",
    "files = [h5py.File('data/random%s.hdf5' % (i+1))['/x'] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<HDF5 dataset \"x\": shape (1000000000,), type \"<f4\">,\n",
       " <HDF5 dataset \"x\": shape (1000000000,), type \"<f4\">]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "arrays = [da.from_array(file,chunks=(10000000, 10000000)) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This creates a disk cache for operations that spill out of disk\n",
    "import chest\n",
    "chest= chest.Chest(available_memory=1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 31.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000059832.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot=arrays[0].dot(arrays[1])\n",
    "dot.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the memory usage for these two arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array sizes: 7812500.0 MB \n"
     ]
    }
   ],
   "source": [
    "print('Array sizes: %s MB ' % ((arrays[0].nbytes+arrays[1].nbytes)/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neither of these fitted in your laptop.\n",
    "You just went big data, yay!\n",
    "\n",
    "### Outputting files\n",
    "**dask** provides methods to export objects s\n",
    "Previous operation was ok because the output only produced one number. If you want to export files greater than memory, **dask** struggles a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What about the outer product?\n",
    "outer=arrays[0]*arrays[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "out = bcolz.zeros(shape=outer.shape, rootdir='output.bcolz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3min 41.4s\n"
     ]
    }
   ],
   "source": [
    "# Store array\n",
    "da.store(outer, out)  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0.02259074,   1.22242928,   0.46698231, ...,   6.49351358,\n",
       "        12.0568552 ,   1.98702204], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer[10000:120000].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't try this at home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 46.4s\n"
     ]
    }
   ],
   "source": [
    "da.to_hdf5('data/product.hdf5', '/x', outer.compute(chest=chest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
